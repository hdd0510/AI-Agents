#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Medical AI Chatbot Launcher - CLEANED & SIMPLIFIED
===========================
Script kh·ªüi ƒë·ªông chatbot v·ªõi logic ƒë√£ ƒë∆∞·ª£c simplified.
"""
# ---- PATCH Pydantic ‚Üî Starlette Request -------------------------------------
from starlette.requests import Request as _StarletteRequest
from pydantic_core import core_schema

def _any_schema(*_):        # ch·∫•p m·ªçi s·ªë ƒë·ªëi s·ªë
    return core_schema.any_schema()

_StarletteRequest.__get_pydantic_core_schema__ = classmethod(_any_schema)
# -----------------------------------------------------------------------------
import argparse
import os
import sys
import json
import time
import logging
from pathlib import Path
os.environ['GRADIO_TEMP_DIR'] = '/tmp'

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class MedicalAIConfig:
    """C·∫•u h√¨nh cho Medical AI Chatbot."""
    
    DEFAULT_CONFIG = {
        "app": {
            "title": "Medical AI Assistant",
            "description": "H·ªá th·ªëng AI h·ªó tr·ª£ ph√¢n t√≠ch h√¨nh ·∫£nh n·ªôi soi",
            "host": "0.0.0.0",
            "port": 8000,
            "share": True,
            "debug": False
        },
        "medical_ai": {
            "device": "cuda",
            "use_reflection": True,
            "detector_model_path": "medical_ai_agents/weights/detect_best.pt",
            "vqa_model_path": "medical_ai_agents/weights/llava-med-mistral-v1.5-7b",
            "modality_classifier_path": "medical_ai_agents/weights/modal_best.pt",
            "region_classifier_path": "medical_ai_agents/weights/location_best.pt"
        },
        "memory": {
            "db_path": "medical_ai_memory.db",
            "short_term_limit": 10,
            "enable_long_term": True,
            "auto_save_important": True
        },
        "ui": {
            "theme": "soft",
            "chat_height": 500,
            "enable_stats": True,
            "enable_history": True,
            "max_file_size": "10MB"
        },
        "security": {
            "enable_user_auth": False,
            "max_sessions": 100,
            "session_timeout": 3600
        }
    }
    
    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        self.config = self.load_config()
    
    def load_config(self) -> dict:
        """Load c·∫•u h√¨nh t·ª´ file ho·∫∑c t·∫°o m·∫∑c ƒë·ªãnh."""
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                
                # Merge with default config
                config = self.DEFAULT_CONFIG.copy()
                self._deep_update(config, user_config)
                return config
            except Exception as e:
                print(f"Error loading config: {e}")
                print("Using default configuration...")
        
        # Save default config
        self.save_config(self.DEFAULT_CONFIG)
        return self.DEFAULT_CONFIG.copy()
    
    def save_config(self, config: dict = None):
        """L∆∞u c·∫•u h√¨nh ra file."""
        config_to_save = config or self.config
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(config_to_save, f, indent=2, ensure_ascii=False)
            print(f"Config saved to {self.config_path}")
        except Exception as e:
            print(f"Error saving config: {e}")
    
    def _deep_update(self, base_dict: dict, update_dict: dict):
        """Deep update dictionary."""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_update(base_dict[key], value)
            else:
                base_dict[key] = value
    
    def get(self, key_path: str, default=None):
        """Get config value using dot notation."""
        keys = key_path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value

def create_enhanced_chatbot():
    """T·∫°o chatbot v·ªõi c·∫•u h√¨nh n√¢ng cao."""
    
    import gradio as gr
    from medical_ai_agents.memory import MedicalAIChatbot, LongShortTermMemory
    from medical_ai_agents import MedicalGraphConfig
    
    # Load config
    config = MedicalAIConfig()
    
    class EnhancedMedicalAIChatbot(MedicalAIChatbot):
        """Chatbot v·ªõi c√°c t√≠nh nƒÉng n√¢ng cao + streaming (SIMPLIFIED)."""
        
        def __init__(self, config: MedicalAIConfig):
            self.app_config = config
            super().__init__()
        
        def _initialize_medical_ai(self):
            """Kh·ªüi t·∫°o Medical AI v·ªõi config t√πy ch·ªânh."""
            medical_config = MedicalGraphConfig(
                device=self.app_config.get("medical_ai.device", "cuda"),
                use_reflection=self.app_config.get("medical_ai.use_reflection", True),
                detector_model_path=self.app_config.get("medical_ai.detector_model_path"),
                vqa_model_path=self.app_config.get("medical_ai.vqa_model_path"),
                modality_classifier_path=self.app_config.get("medical_ai.modality_classifier_path"),
                region_classifier_path=self.app_config.get("medical_ai.region_classifier_path")
            )
            
            from medical_ai_agents import MedicalAISystem
            return MedicalAISystem(medical_config)
        
        def process_message_streaming(self, message, image, history, username, session_state):
            """SIMPLIFIED streaming version - cleaned up logic."""
            import uuid
            
            # Generate session and user IDs
            session_id = session_state.get("session_id", str(uuid.uuid4()))
            user_id = self.generate_user_id(username)
            
            session_state["session_id"] = session_id
            session_state["user_id"] = user_id
            
            # Start with user message
            history.append([message, "ü§î ƒêang ph√¢n t√≠ch..."])
            yield "", history, session_state
            
            try:
                # Get contextual information
                context = self.memory.get_contextual_prompt(session_id, user_id)
                
                # SIMPLIFIED: Determine processing mode
                has_image = image is not None
                
                if has_image:
                    result = self.medical_ai.analyze(image_path=image, query=message)
                    
                    if result.get("success", False):
                        response_parts = [f"üîç **Analysis Results:**\n{result.get('final_answer', '')}"]
                        
                        # FIXED: Check for visualization in agent_results
                        if "agent_results" in result and "detector_result" in result["agent_results"]:
                            detector = result["agent_results"]["detector_result"]
                            
                            if detector.get("visualization_available") and detector.get("visualization_base64"):
                                # Create proper HTML img tag
                                img_b64 = detector["visualization_base64"]
                                img_html = f'''
        <div style="margin: 10px 0; text-align: center;">
            <p><strong>üéØ Visualization Result:</strong></p>
            <img src="data:image/png;base64,{img_b64}" 
                alt="Polyp Detection Results" 
                style="max-width: 100%; height: auto; border-radius: 8px; 
                        box-shadow: 0 2px 8px rgba(0,0,0,0.1); border: 1px solid #ddd;">
        </div>'''
                                response_parts.append(img_html)
                                
                                # Also save to session state for separate display
                                session_state["last_visualization"] = img_b64
                        
                        final_response = "\n\n".join(response_parts)
                        history[-1][1] = final_response
                        yield "", history, session_state
                                
                    else:
                        error_response = "‚ùå C√≥ l·ªói trong qu√° tr√¨nh ph√¢n t√≠ch h√¨nh ·∫£nh.\n"
                        error_response += f"Chi ti·∫øt l·ªói: {result.get('error', 'Unknown error')}"
                        history[-1][1] = error_response
                        yield "", history, session_state
                
                else:
                    # =============  TEXT-ONLY WORKFLOW (SIMPLIFIED) =============
                    logger.info(f"Processing text-only query: '{message[:50]}...'")
                    
                    history[-1][1] = "üß† ƒêang t∆∞ v·∫•n qua LLaVA..."
                    yield "", history, session_state
                    time.sleep(0.3)
                    
                    # SIMPLIFIED: Call medical AI with clean parameters
                    result = self.medical_ai.analyze(
                        image_path=None,  # No image - triggers text-only mode
                        query=message,
                        medical_context={
                            "user_context": context,
                            "is_text_only": True
                        } if context else {"is_text_only": True}
                    )
                    
                    logger.debug(f"Text-only result: success={result.get('success', False)}")
                    
                    if result.get("success", False):
                        # Stream LLaVA text-only response
                        history[-1][1] = "üìù ƒêang t·∫°o t∆∞ v·∫•n..."
                        yield "", history, session_state
                        time.sleep(0.3)
                        
                        # Check if VQA result is successful
                        vqa_success = True
                        if "agent_results" in result and "vqa_result" in result["agent_results"]:
                            vqa_result = result["agent_results"]["vqa_result"]
                            vqa_success = vqa_result.get("success", False)
                            
                            if not vqa_success:
                                # VQA/LLaVA failed - show safety error
                                error_response = "‚ùå **H·ªá th·ªëng t∆∞ v·∫•n y t·∫ø g·∫∑p s·ª± c·ªë**\n\n"
                                error_response += vqa_result.get("answer", "L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh t∆∞ v·∫•n.")
                                error_response += "\n\nüîÑ **Vui l√≤ng:**\n"
                                error_response += "- Th·ª≠ l·∫°i sau v√†i ph√∫t\n"
                                error_response += "- Ho·∫∑c tham kh·∫£o b√°c sƒ© tr·ª±c ti·∫øp n·∫øu c·∫ßn thi·∫øt"
                                
                                history[-1][1] = error_response
                                yield "", history, session_state
                                return  # Exit early
                        
                        # VQA succeeded - process response
                        if vqa_success and "final_answer" in result:
                            final_answer = result["final_answer"]
                            streaming_text = "üß† **T∆∞ v·∫•n y t·∫ø qua LLaVA:**\n\n"
                            
                            # Add context if available
                            if context:
                                streaming_text += "üí≠ **D·ª±a tr√™n th√¥ng tin tr∆∞·ªõc ƒë√≥:**\n"
                                streaming_text += (context[:200] + "..." if len(context) > 200 else context) + "\n\n"
                            
                            # Stream the LLaVA medical consultation
                            words = final_answer.split()
                            for i, word in enumerate(words):
                                streaming_text += word + " "
                                if i % 3 == 0:  # Update every 3 words for smoother streaming
                                    history[-1][1] = streaming_text
                                    yield "", history, session_state
                                    time.sleep(0.05)
                            
                            # Add LLaVA processing info
                            streaming_text += f"\n\nüî¨ **ƒê∆∞·ª£c x·ª≠ l√Ω b·ªüi:** LLaVA-Med (Text-Only Mode)"
                            streaming_text += f"\nüìä **Lo·∫°i t∆∞ v·∫•n:** Medical consultation without image"
                            
                            # Add medical disclaimer
                            streaming_text += f"\n\n‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng:**"
                            streaming_text += f"\n- ƒê√¢y l√† t∆∞ v·∫•n s∆° b·ªô t·ª´ AI, kh√¥ng thay th·∫ø kh√°m tr·ª±c ti·∫øp"
                            streaming_text += f"\n- H√£y tham kh·∫£o √Ω ki·∫øn b√°c sƒ© chuy√™n khoa ƒë·ªÉ c√≥ ch·∫©n ƒëo√°n ch√≠nh x√°c"
                            streaming_text += f"\n- N·∫øu c√≥ tri·ªáu ch·ª©ng nghi√™m tr·ªçng, h√£y ƒë·∫øn c∆° s·ªü y t·∫ø ngay l·∫≠p t·ª©c"
                            
                            history[-1][1] = streaming_text.strip()
                            yield "", history, session_state
                            
                        else:
                            # Fallback handling with safety checks
                            fallback_response = self._create_safe_fallback_response(result, context, message)
                            history[-1][1] = fallback_response
                            yield "", history, session_state
                    
                    else:
                        # Handle text-only system error
                        logger.error(f"Medical AI system failed: {result.get('error', 'Unknown error')}")
                        error_response = self._create_system_error_response(message)
                        history[-1][1] = error_response
                        yield "", history, session_state
                
                # Save to memory
                final_response = history[-1][1]
                interaction = {
                    "query": message,
                    "response": final_response,
                    "has_image": has_image,
                    "analysis": result if 'result' in locals() else None,
                    "polyp_count": result.get("polyp_count", 0) if 'result' in locals() else 0,
                    "is_text_only": not has_image
                }
                
                logger.debug(f"Saving interaction to memory: query='{message[:30]}...', has_image={has_image}")
                self.memory.add_to_short_term(session_id, interaction)
                
                # Save important interactions to long term
                if has_image or "polyp" in message.lower() or "y t·∫ø" in message.lower():
                    logger.info(f"Saving important interaction to long-term memory for user {user_id}")
                    self.memory.save_to_long_term(user_id, session_id, interaction)
                
            except Exception as e:
                logger.error(f"Error in process_message_streaming: {str(e)}", exc_info=True)
                error_response = f"‚ùå Xin l·ªói, c√≥ l·ªói h·ªá th·ªëng x·∫£y ra: {str(e)}"
                history[-1][1] = error_response
                yield "", history, session_state
        
        def _create_safe_fallback_response(self, result, context, message):
            """Create safe fallback response for text-only queries."""
            fallback_response = "üß† **T∆∞ v·∫•n y t·∫ø:**\n\n"
            
            # Check for VQA result first
            if "agent_results" in result and "vqa_result" in result["agent_results"]:
                vqa_result = result["agent_results"]["vqa_result"]
                if vqa_result.get("success", False):
                    llava_answer = vqa_result.get("answer", "")
                    if llava_answer and len(llava_answer.strip()) > 20:
                        fallback_response += llava_answer
                        if context:
                            fallback_response = f"üí≠ **D·ª±a tr√™n th√¥ng tin tr∆∞·ªõc ƒë√≥:**\n{context[:200]}...\n\n" + fallback_response
                        
                        fallback_response += f"\n\nüî¨ **ƒê∆∞·ª£c x·ª≠ l√Ω b·ªüi:** LLaVA-Med (Text-Only Mode)"
                        fallback_response += f"\n\n‚ö†Ô∏è **L∆∞u √Ω:** ƒê√¢y l√† t∆∞ v·∫•n AI, h√£y tham kh·∫£o b√°c sƒ© chuy√™n khoa."
                        return fallback_response
            
            # Generic helpful response
            if context:
                fallback_response += f"üí≠ **D·ª±a tr√™n th√¥ng tin tr∆∞·ªõc ƒë√≥:**\n{context[:200]}...\n\n"
            
            # Customize based on message content
            if any(greeting in message.lower() for greeting in ["hello", "hi", "xin ch√†o", "ch√†o"]):
                fallback_response += "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI y t·∫ø chuy√™n h·ªó tr·ª£ ph√¢n t√≠ch h√¨nh ·∫£nh n·ªôi soi.\n\n"
                fallback_response += "üî¨ **T√¥i c√≥ th·ªÉ gi√∫p b·∫°n:**\n"
                fallback_response += "- Ph√¢n t√≠ch h√¨nh ·∫£nh n·ªôi soi ƒë·∫°i tr√†ng\n"
                fallback_response += "- Ph√°t hi·ªán polyp v√† c√°c b·∫•t th∆∞·ªùng\n"
                fallback_response += "- Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ y t·∫ø ti√™u h√≥a\n\n"
                fallback_response += "B·∫°n c√≥ th·ªÉ t·∫£i l√™n h√¨nh ·∫£nh n·ªôi soi ho·∫∑c ƒë·∫∑t c√¢u h·ªèi c·ª• th·ªÉ ƒë·ªÉ t√¥i h·ªó tr·ª£ t·ªët h∆°n."
            else:
                fallback_response += "C·∫£m ∆°n b·∫°n ƒë√£ ƒë∆∞a ra c√¢u h·ªèi. ƒê·ªÉ t√¥i c√≥ th·ªÉ h·ªó tr·ª£ t·ªët nh·∫•t:\n\n"
                fallback_response += "üìã **Khuy·∫øn ngh·ªã:**\n"
                fallback_response += "1. M√¥ t·∫£ chi ti·∫øt h∆°n v·ªÅ tri·ªáu ch·ª©ng b·∫°n g·∫∑p ph·∫£i\n"
                fallback_response += "2. T·∫£i l√™n h√¨nh ·∫£nh n·ªôi soi n·∫øu c√≥\n"
                fallback_response += "3. ƒê·∫∑t c√¢u h·ªèi c·ª• th·ªÉ v·ªÅ v·∫•n ƒë·ªÅ s·ª©c kh·ªèe\n\n"
                fallback_response += "üè• **L∆∞u √Ω:** T√¥i l√† tr·ª£ l√Ω AI h·ªó tr·ª£, kh√¥ng thay th·∫ø kh√°m b√°c sƒ©."
            
            return fallback_response
        
        def _create_system_error_response(self, message):
            """Create system error response with helpful guidance."""
            error_response = "‚ùå **H·ªá th·ªëng t·∫°m th·ªùi g·∫∑p s·ª± c·ªë**\n\n"
            error_response += "Xin l·ªói v√¨ s·ª± b·∫•t ti·ªán n√†y. H·ªá th·ªëng LLaVA hi·ªán kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n.\n\n"
            error_response += "üîÑ **B·∫°n c√≥ th·ªÉ:**\n"
            error_response += "- Th·ª≠ l·∫°i sau v√†i ph√∫t\n"
            error_response += "- ƒê·∫∑t l·∫°i c√¢u h·ªèi v·ªõi t·ª´ ng·ªØ kh√°c\n"
            error_response += "- T·∫£i l√™n h√¨nh ·∫£nh ƒë·ªÉ ph√¢n t√≠ch tr·ª±c quan\n\n"
            error_response += "üè• **N·∫øu c·∫ßn t∆∞ v·∫•n g·∫•p:** Vui l√≤ng li√™n h·ªá b√°c sƒ© chuy√™n khoa tr·ª±c ti·∫øp."
            return error_response

        def create_enhanced_interface(self):
            """T·∫°o giao di·ªán v·ªõi nhi·ªÅu t√≠nh nƒÉng h∆°n."""
            
            # Custom CSS with image support
            custom_css = """
            .main-container { 
                max-width: 1200px; 
                margin: 0 auto;
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            }
            .header-section {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 1.5rem;
                border-radius: 10px;
                margin-bottom: 1.5rem;
                text-align: center;
            }
            .chat-container { 
                height: 600px; 
                border-radius: 10px;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                margin-bottom: 1rem;
                width: 100%;
            }
            /* FIXED: C·∫£i thi·ªán hi·ªÉn th·ªã ·∫£nh trong chat */
            .chat-container img {
                max-width: 80% !important;
                max-height: 400px !important;
                border-radius: 8px !important;
                margin: 10px auto !important;
                display: block !important;
                box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2) !important;
                border: 1px solid rgba(0, 0, 0, 0.1) !important;
            }
            /* Ensure images in message bubble are properly styled */
            .message img {
                max-width: 100% !important;
                height: auto !important;
                border-radius: 8px !important;
                margin: 10px 0 !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1) !important;
            }
            .upload-container { 
                border: 2px dashed #4CAF50; 
                padding: 20px; 
                text-align: center;
                border-radius: 10px;
                background: #f8f9fa;
                margin-bottom: 1rem;
            }
            .chat-input-container {
                display: flex;
                width: 100%;
                margin: 0.5rem 0;
                gap: 0.5rem;
            }
            .tab-container {
                margin-top: 1rem;
                border-radius: 10px;
                overflow: hidden;
                box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            }
            .button-row {
                display: flex;
                gap: 0.5rem;
                margin: 0.5rem 0;
            }
            """
            
            with gr.Blocks(
                title=self.app_config.get("app.title", "Medical AI Assistant"),
                theme=gr.themes.Soft() if self.app_config.get("ui.theme") == "soft" else gr.themes.Default(),
                css=custom_css
            ) as interface:
                
                # State management
                session_state = gr.State({})
                
                # Header
                with gr.Row(elem_classes=["header-section"]):
                    gr.Markdown(f"""
                    # üè• {self.app_config.get("app.title", "Medical AI Assistant")}
                    ### {self.app_config.get("app.description", "H·ªá th·ªëng AI h·ªó tr·ª£ ph√¢n t√≠ch h√¨nh ·∫£nh n·ªôi soi")}
                    
                    **üéØ T√≠nh nƒÉng n·ªïi b·∫≠t:**
                    - üß† **LLaVA-Med Integration**: S·ª≠ d·ª•ng AI chuy√™n v·ªÅ y t·∫ø
                    - üîç **Ph√¢n t√≠ch ch√≠nh x√°c**: AI ƒëa agent v·ªõi ƒë·ªô tin c·∫≠y cao
                    - üí¨ **T∆∞ v·∫•n th√¥ng minh**: H·ªó tr·ª£ c·∫£ h√¨nh ·∫£nh v√† text-only
                    - üìä **Streaming response**: Ph·∫£n h·ªìi real-time
                    """)
                
                with gr.Row():
                    # Main chat interface in a centered column
                    with gr.Column(scale=1, min_width=800):
                        # Chat container - FIXED: Enable HTML rendering for images
                        chatbot = gr.Chatbot(
                            label="üí¨ Cu·ªôc tr√≤ chuy·ªán v·ªõi AI",
                            height=self.app_config.get("ui.chat_height", 650),
                            show_copy_button=True,
                            elem_classes=["chat-container"],
                            layout="bubble",
                            render_markdown=True,
                            sanitize_html=False,  # FIXED: Allow HTML images
                        )
                        
                        # Input and buttons in a clean layout
                        with gr.Row(elem_classes=["chat-input-container"]):
                            msg_input = gr.Textbox(
                                placeholder="üí≠ H√£y m√¥ t·∫£ tri·ªáu ch·ª©ng ho·∫∑c ƒë·∫∑t c√¢u h·ªèi v·ªÅ h√¨nh ·∫£nh...",
                                label="Tin nh·∫Øn c·ªßa b·∫°n",
                                scale=5,
                                lines=2
                            )
                            with gr.Column(scale=1, elem_classes=["button-row"]):
                                send_btn = gr.Button("üì§ G·ª≠i", variant="primary", size="lg")
                                clear_btn = gr.Button("üóëÔ∏è X√≥a", variant="stop", size="lg")
                        
                        # Tabs for image upload and results
                        tabs = gr.Tabs(elem_classes=["tab-container"])
                        with tabs:
                            with gr.TabItem("üñºÔ∏è T·∫£i ·∫£nh"):
                                # Advanced image upload
                                image_input = gr.Image(
                                    label="Ch·ªçn h√¨nh ·∫£nh n·ªôi soi, X-quang ho·∫∑c h√¨nh ·∫£nh y t·∫ø kh√°c",
                                    type="filepath",
                                    elem_classes=["upload-container"]
                                )
                                
                                gr.Markdown("""
                                **L∆∞u √Ω:** 
                                - H·ªó tr·ª£ ƒë·ªãnh d·∫°ng: JPG, PNG, DICOM
                                - K√≠ch th∆∞·ªõc t·ªëi ƒëa: 10MB
                                - ƒê·∫£m b·∫£o h√¨nh ·∫£nh r√µ n√©t cho k·∫øt qu·∫£ t·ªët nh·∫•t
                                - **·∫¢nh k·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã tr·ª±c ti·∫øp trong chat**
                                """)
                            
                            with gr.TabItem("üìä K·∫øt qu·∫£ ph√°t hi·ªán"):
                                result_image = gr.Image(
                                    label="K·∫øt qu·∫£ ph√°t hi·ªán polyp",
                                    type="filepath",
                                    interactive=False
                                )
                                
                                show_latest_result_btn = gr.Button("üîÑ Hi·ªÉn th·ªã k·∫øt qu·∫£ m·ªõi nh·∫•t", variant="secondary")
                
                # Hidden state for username (required for functions)
                username_input = gr.Textbox(value="B·ªánh nh√¢n", visible=False)
                user_info = gr.Textbox(value="", visible=False)
                
                # FIXED: Event handlers v·ªõi streaming support
                def safe_process_message_streaming(message, image, history, username, user_info, state):
                    """Wrapper cho streaming function."""
                    try:
                        # ƒê·∫£m b·∫£o state lu√¥n l√† dict
                        if state is None:
                            state = {}
                        # Add user info to medical context
                        if user_info.strip():
                            if "medical_context" not in state:
                                state["medical_context"] = {}
                            state["medical_context"]["user_info"] = user_info
                        
                        # Use streaming version
                        for msg, hist, updated_state in self.process_message_streaming(message, image, history, username, state):
                            yield msg, hist, updated_state, None
                        
                        # Check for visualization result
                        if "last_result_image_data" in updated_state:
                            yield msg, hist, updated_state, None
                            
                    except Exception as e:
                        logger.error(f"Error in safe_process_message_streaming: {str(e)}", exc_info=True)
                        error_msg = f"‚ùå L·ªói x·ª≠ l√Ω: {str(e)}"
                        if history:
                            history[-1][1] = error_msg
                        else:
                            history.append([message, error_msg])
                        yield "", history, state, None

                def show_latest_visualization(state):
                    """Hi·ªÉn th·ªã k·∫øt qu·∫£ ph√°t hi·ªán m·ªõi nh·∫•t t·ª´ session state"""
                    try:
                        if "viz_image_path" in state and state["viz_image_path"]:
                            return state["viz_image_path"]
                        else:
                            return None
                    except Exception as e:
                        logger.error(f"Error showing visualization: {str(e)}")
                        return None
                
                # Connect all events v·ªõi streaming
                send_btn.click(
                    safe_process_message_streaming,
                    inputs=[msg_input, image_input, chatbot, username_input, user_info, session_state],
                    outputs=[msg_input, chatbot, session_state, result_image]
                )
                
                msg_input.submit(
                    safe_process_message_streaming,
                    inputs=[msg_input, image_input, chatbot, username_input, user_info, session_state],
                    outputs=[msg_input, chatbot, session_state, result_image]
                )
                
                show_latest_result_btn.click(
                    show_latest_visualization,
                    inputs=[session_state],
                    outputs=[result_image]
                ).then(
                    lambda: gr.update(selected=1),  # Lu√¥n chuy·ªÉn sang tab k·∫øt qu·∫£
                    outputs=[tabs]
                )
                
                clear_btn.click(lambda: [], outputs=[chatbot])
            
            return interface
    
    # Create and return enhanced chatbot
    return EnhancedMedicalAIChatbot(config)

def main():
    """H√†m main v·ªõi argument parsing."""
    parser = argparse.ArgumentParser(description="Medical AI Chatbot Launcher")
    parser.add_argument("--config", default="config.json", help="Path to config file")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind")
    parser.add_argument("--share", action="store_true", help="Create shareable link")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    parser.add_argument("--device", choices=["cuda", "cpu"], help="Device to use")
    
    args = parser.parse_args()
    
    # Load config
    config = MedicalAIConfig(args.config)
    
    # Override config with command line args
    if args.host:
        config.config["app"]["host"] = args.host
    if args.port:
        config.config["app"]["port"] = args.port
    if args.share:
        config.config["app"]["share"] = True
    if args.debug:
        config.config["app"]["debug"] = True
    if args.device:
        config.config["medical_ai"]["device"] = args.device
    
    print("üöÄ Starting Medical AI Chatbot...")
    print(f"üìç Host: {config.get('app.host')}")
    print(f"üîå Port: {config.get('app.port')}")
    print(f"üåê Share: {config.get('app.share')}")
    print(f"üñ•Ô∏è  Device: {config.get('medical_ai.device')}")
    print(f"üé¨ Features: Simplified Logic + LLaVA Integration")
    
    try:
        # Create enhanced chatbot
        chatbot = create_enhanced_chatbot()
        interface = chatbot.create_enhanced_interface()
        
        # Launch interface
        interface.launch(
            server_name=config.get("app.host"),
            server_port=config.get("app.port"),
            share=config.get("app.share"),
            debug=config.get("app.debug"),
            show_error=True,
        )
        
    except Exception as e:
        print(f"‚ùå Error launching chatbot: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()