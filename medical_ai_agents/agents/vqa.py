#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Medical AI Agents - VQA Agent (MODIFIED: Always Use LLaVA)
---------------------------
Agent luÃ´n sá»­ dá»¥ng LLaVA tool cho cáº£ image vÃ  text-only queries.
"""

import json
import os
from typing import Dict, Any, List
import logging
import re
import base64

from medical_ai_agents.agents.base_agent import BaseAgent
from medical_ai_agents.tools.base_tools import BaseTool
from medical_ai_agents.tools.vqa.llava_tools import LLaVATool
from langchain.schema import SystemMessage, HumanMessage

class VQAAgent(BaseAgent):
    """Agent luÃ´n sá»­ dá»¥ng LLaVA tool cho má»i queries (cÃ³ hoáº·c khÃ´ng cÃ³ hÃ¬nh áº£nh)."""
    
    def __init__(self, model_path: str, llm_model: str = "gpt-4o-mini", device: str = "cuda"):
        """
        Khá»Ÿi táº¡o VQA Agent vá»›i LLM controller.
        
        Args:
            model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n LLaVA model
            llm_model: MÃ´ hÃ¬nh LLM sá»­ dá»¥ng lÃ m controller
            device: Device Ä‘á»ƒ cháº¡y model (cuda/cpu)
        """
        # Set attributes before super().__init__()
        self.model_path = model_path
        
        super().__init__(name="VQA Agent", llm_model=llm_model, device=device)
        self.llava_tool = None
    
    def _register_tools(self) -> List[BaseTool]:
        """Register tools for this agent."""
        self.llava_tool = LLaVATool(model_path=self.model_path, device=self.device)
        return [self.llava_tool]
    
    def _get_system_prompt(self) -> str:
        """Get the system prompt that defines this agent's role."""
        prompt = """
Báº¡n lÃ  má»™t AI chuyÃªn gia y táº¿ vá»›i kháº£ nÄƒng sá»­ dá»¥ng LLaVA (Large Language and Vision Assistant) Ä‘á»ƒ:

1. **PhÃ¢n tÃ­ch hÃ¬nh áº£nh y táº¿** khi cÃ³ hÃ¬nh áº£nh
2. **TÆ° váº¥n y táº¿ chuyÃªn sÃ¢u** khi chá»‰ cÃ³ text (text-only)

**CÃ´ng cá»¥ chÃ­nh:**
- `llava_vqa`: CÃ´ng cá»¥ LLaVA cÃ³ thá»ƒ xá»­ lÃ½:
  - **Image + Text**: PhÃ¢n tÃ­ch hÃ¬nh áº£nh ná»™i soi, X-quang, CT, MRI...
  - **Text Only**: TÆ° váº¥n y táº¿ dá»±a trÃªn kiáº¿n thá»©c chuyÃªn mÃ´n cá»§a LLaVA

**Quy trÃ¬nh lÃ m viá»‡c - LUÃ”N Sá»¬ Dá»¤NG LLAVA:**

1. **XÃ¡c Ä‘á»‹nh loáº¡i query:**
   - CÃ³ hÃ¬nh áº£nh: PhÃ¢n tÃ­ch image + answer question
   - Chá»‰ text: Medical consultation using LLaVA's knowledge

2. **Sá»­ dá»¥ng LLaVA tool:**
   ```
   Tool: llava_vqa
   Parameters: {"query": "user's question", "image_path": "path/to/image.jpg" (náº¿u cÃ³) hoáº·c null (náº¿u text-only), "medical_context": {context_from_other_agents}}
   ```

3. **PhÃ¢n tÃ­ch káº¿t quáº£:**
   - ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng pháº£n há»“i tá»« LLaVA
   - Bá»• sung thÃ´ng tin y khoa náº¿u cáº§n
   - ÄÆ°a ra khuyáº¿n nghá»‹ phÃ¹ há»£p

**Æ¯u Ä‘iá»ƒm cá»§a viá»‡c luÃ´n sá»­ dá»¥ng LLaVA:**
- **Consistency**: CÃ¹ng má»™t model cho cáº£ image vÃ  text
- **Medical Knowledge**: LLaVA-Med cÃ³ kiáº¿n thá»©c y khoa sÃ¢u
- **Contextual Understanding**: Hiá»ƒu context tá»‘t hÆ¡n traditional LLM
- **Unified Experience**: User experience nháº¥t quÃ¡n

**LÆ°u Ã½ quan trá»ng:**
- LUÃ”N gá»i llava_vqa tool, khÃ´ng bao giá» tráº£ lá»i trá»±c tiáº¿p
- Náº¿u khÃ´ng cÃ³ image_path, pass null hoáº·c khÃ´ng include parameter Ä‘Ã³
- LLaVA sáº½ tá»± Ä‘á»™ng chuyá»ƒn sang text-only mode
- Váº«n khuyáº¿n nghá»‹ khÃ¡m trá»±c tiáº¿p khi cáº§n thiáº¿t
"""
        return prompt

    def initialize(self) -> bool:
        """Khá»Ÿi táº¡o agent vÃ  cÃ¡c cÃ´ng cá»¥."""
        try:
            # Tools are already initialized in _register_tools
            self.initialized = True
            return True
        except Exception as e:
            self.logger.error(f"Failed to initialize VQA agent: {str(e)}")
            self.initialized = False
            return False
    
    def _extract_task_input(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Extract task-specific input from state."""
        # Get detector results if available
        detector_result = state.get("detector_result", {})
        modality_result = state.get("modality_result", {})
        region_result = state.get("region_result", {})
        
        medical_context = {}
        
        # Add detection info to context
        if detector_result and detector_result.get("success", False):
            objects = detector_result.get("objects", [])
            medical_context["detected_polyps"] = len(objects)
            
            if objects:
                polyp_descriptions = []
                for i, obj in enumerate(objects):
                    desc = f"Polyp {i+1}: {obj.get('confidence', 0):.2f} confidence, "
                    desc += f"location: {obj.get('position_description', 'unknown')}"
                    polyp_descriptions.append(desc)
                
                medical_context["polyp_details"] = "; ".join(polyp_descriptions)
        
        # Add modality info
        if modality_result and modality_result.get("success", False):
            medical_context["imaging_modality"] = modality_result.get("class_name", "Unknown")
        
        # Add region info
        if region_result and region_result.get("success", False):
            medical_context["anatomical_region"] = region_result.get("class_name", "Unknown")
        
        # Add user-provided context
        user_context = state.get("medical_context", {})
        if user_context:
            medical_context.update(user_context)
        
        return {
            "image_path": state.get("image_path", ""),
            "query": state.get("query", ""),
            "medical_context": medical_context,
            "is_text_only": state.get("is_text_only", False)
        }
    
    
    def _format_task_input(self, task_input: Dict[str, Any]) -> str:
        """Format task input for LLM prompt."""
        image_path = task_input.get("image_path", "")
        query = task_input.get("query", "")
        context = task_input.get("medical_context", {})
        is_text_only = task_input.get("is_text_only", False)
        
        context_str = "\n".join([f"- {k}: {v}" for k, v in context.items()]) if context else "None"
        
        # SIMPLIFIED: LuÃ´n sá»­ dá»¥ng "query" parameter
        if is_text_only or not image_path or not os.path.exists(image_path):
            return f"""**TEXT-ONLY MEDICAL CONSULTATION**

CÃ¢u há»i cá»§a bá»‡nh nhÃ¢n: "{query}"

ThÃ´ng tin y táº¿ bá»• sung:
{context_str}

HÃ£y sá»­ dá»¥ng LLaVA tool Ä‘á»ƒ tÆ° váº¥n y táº¿ chuyÃªn sÃ¢u (text-only mode):

Tool: llava_vqa
Parameters: {{"query": "{query}", "medical_context": {json.dumps(context)}}}

LLaVA sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng kiáº¿n thá»©c y khoa Ä‘á»ƒ tÆ° váº¥n khi khÃ´ng cÃ³ hÃ¬nh áº£nh.
"""
        else:
            return f"""**IMAGE-BASED MEDICAL ANALYSIS**

HÃ¬nh áº£nh cáº§n phÃ¢n tÃ­ch: {image_path}
CÃ¢u há»i: "{query if query else 'PhÃ¢n tÃ­ch hÃ¬nh áº£nh y táº¿ nÃ y'}"

ThÃ´ng tin y táº¿ bá»• sung:
{context_str}

HÃ£y sá»­ dá»¥ng LLaVA tool Ä‘á»ƒ phÃ¢n tÃ­ch hÃ¬nh áº£nh vÃ  tráº£ lá»i cÃ¢u há»i:

Tool: llava_vqa
Parameters: {{"query": "{query if query else 'PhÃ¢n tÃ­ch hÃ¬nh áº£nh y táº¿ nÃ y'}", "image_path": "{image_path}", "medical_context": {json.dumps(context)}}}
"""
    
    def _format_synthesis_input(self) -> str:
        return """
**SYNTHESIS TASK: LLaVA Result Processing**

Báº¡n Ä‘Ã£ nháº­n Ä‘Æ°á»£c káº¿t quáº£ tá»« LLaVA tool. HÃ£y tá»•ng há»£p vÃ  Ä‘Æ°a ra pháº£n há»“i cuá»‘i cÃ¹ng:

**YÃªu cáº§u output:**
1. **PhÃ¢n tÃ­ch káº¿t quáº£ LLaVA**: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng vÃ  Ä‘á»™ tin cáº­y
2. **Bá»• sung thÃ´ng tin**: ThÃªm kiáº¿n thá»©c y khoa náº¿u cáº§n
3. **Khuyáº¿n nghá»‹**: Lá»i khuyÃªn y táº¿ phÃ¹ há»£p
4. **Disclaimer**: LÆ°u Ã½ vá» giá»›i háº¡n cá»§a AI vÃ  khuyáº¿n nghá»‹ khÃ¡m trá»±c tiáº¿p

**LÆ°u Ã½:**
- LLaVA Ä‘Ã£ xá»­ lÃ½ query (image-based hoáº·c text-only)
- Báº¡n cáº§n synthesis Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i hoÃ n chá»‰nh
- Äáº£m báº£o tÃ­nh chuyÃªn nghiá»‡p vÃ  an toÃ n trong y táº¿

**Output format:**
```json
{
    "vqa_result": {
        "success": true/false,
        "answer": "cÃ¢u tráº£ lá»i hoÃ n chá»‰nh sau khi synthesis káº¿t quáº£ LLaVA",
        "analysis": "phÃ¢n tÃ­ch vá» káº¿t quáº£ tá»« LLaVA vÃ  bá»• sung thÃ´ng tin",
        "llava_response": "original response from LLaVA",
        "query_type": "image_based/text_only",
        "confidence": 0.0-1.0
    }
}
```
"""
    
    def _parse_tool_calls(self, plan: str) -> List[Dict[str, Any]]:
        """SIMPLIFIED parsing for LLaVA tool calls."""
        tool_calls = []
        
        self.logger.info(f"[VQA] Parsing LLM plan:\n{plan}")
        
        # Method 1: Standard format parsing
        lines = plan.split("\n")
        current_tool = None
        current_params = {}
        
        for line in lines:
            line = line.strip()
            if line.startswith("Tool:"):
                # Save previous tool if exists
                if current_tool:
                    tool_calls.append({
                        "tool_name": current_tool,
                        "params": current_params
                    })
                
                # Start new tool
                current_tool = line.replace("Tool:", "").strip()
                current_params = {}
            
            elif line.startswith("Parameters:"):
                # Try to parse JSON parameters
                try:
                    params_text = line.replace("Parameters:", "").strip()
                    if params_text.startswith("{") and params_text.endswith("}"):
                        current_params = json.loads(params_text)
                except Exception as e:
                    self.logger.warning(f"Failed to parse parameters: {line}, error: {e}")
        
        # Add last tool
        if current_tool:
            tool_calls.append({
                "tool_name": current_tool,
                "params": current_params
            })
        
        # Method 2: Regex extraction as fallback
        if not tool_calls:
            llava_pattern = r'llava_vqa.*?[\{]([^}]+)[\}]'
            matches = re.findall(llava_pattern, plan, re.DOTALL | re.IGNORECASE)
            
            for match in matches:
                try:
                    param_str = "{" + match + "}"
                    params = json.loads(param_str)
                    tool_calls.append({
                        "tool_name": "llava_vqa",
                        "params": params
                    })
                except Exception as e:
                    self.logger.warning(f"Failed to parse regex match: {match}, error: {e}")
        
        self.logger.info(f"[VQA] Parsed {len(tool_calls)} tool calls")
        return tool_calls
    
    def _extract_agent_result(self, synthesis: str) -> Dict[str, Any]:
        """Extract agent result from LLM synthesis."""
        try:
            # Try to extract JSON from synthesis
            json_start = synthesis.find('{')
            json_end = synthesis.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = synthesis[json_start:json_end]
                vqa_result = json.loads(json_str)
                self.logger.info(f"[VQA] Successfully extracted JSON result")
                return vqa_result
            
            # If no JSON found, create result from synthesis text
            self.logger.warning("[VQA] No JSON found in synthesis, parsing text")
            
            # Try to extract answer from text
            answer = ""
            # Look for answer patterns
            answer_patterns = [
                r'answer["\s]*:["\s]*([^"\n]+)',
                r'Answer[:\s]+([^\n]+)',
                r'cÃ¢u tráº£ lá»i[:\s]+([^\n]+)',
            ]
            for pattern in answer_patterns:
                matches = re.findall(pattern, synthesis, re.IGNORECASE)
                if matches:
                    answer = matches[0].strip()
                    break
            
            # If still no answer found, use the whole synthesis as answer
            if not answer:
                answer = synthesis
                
            # Fallback: Create result from synthesis text
            return {
                "vqa_result": {
                    "success": True,
                    "answer": answer,
                    "analysis": "Generated from LLaVA tool execution",
                    "query_type": "llava_processed"
                }
            }
        except Exception as e:
            self.logger.error(f"[VQA] Failed to extract agent result: {str(e)}")
            return {
                "vqa_result": {
                    "success": False,
                    "error": str(e),
                    "answer": synthesis
                }
            }

    def _process_state(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """SIMPLIFIED process state - no complex fallbacks needed."""
        # Extract relevant information from state
        task_input = self._extract_task_input(state)
        self.logger.info(f"[VQA] Processing task: {json.dumps(task_input, indent=2)}")
        
        # Let LLM decide how to use LLaVA tool
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=self._format_task_input(task_input))
        ]
        
        # Get response from LLM
        response = self.llm.invoke(messages)
        plan = response.content
        self.logger.info(f"[VQA] LLM plan: {plan}")
        
        # Parse the plan and execute LLaVA tool
        tool_calls = self._parse_tool_calls(plan)
        self.logger.info(f"[VQA] Parsed tool calls: {json.dumps(tool_calls, indent=2)}")
        
        # SIMPLE FALLBACK: If no tool calls, create one from task_input
        if not tool_calls:
            self.logger.warning("[VQA] No tool calls found, creating simple fallback")
            fallback_params = {"query": task_input.get("query", "Medical consultation")}
            
            # Add image_path if available
            if task_input.get("image_path") and os.path.exists(task_input["image_path"]):
                fallback_params["image_path"] = task_input["image_path"]
            
            # Add medical context
            if task_input.get("medical_context"):
                fallback_params["medical_context"] = task_input["medical_context"]
            
            tool_calls = [{"tool_name": "llava_vqa", "params": fallback_params}]
            self.logger.info(f"[VQA] Simple fallback created")
        
        # Execute tool calls
        results = {}
        llava_success = False
        
        for tool_call in tool_calls:
            tool_name = tool_call.get("tool_name")
            params = tool_call.get("params", {})
            
            if tool_name == "llava_vqa":
                self.logger.info(f"[VQA] Executing LLaVA tool with params: {json.dumps(params, indent=2)}")
                
                # Execute LLaVA tool
                tool_result = self.execute_tool(tool_name, **params)
                results[tool_name] = tool_result
                
                # Check success and response quality
                llava_success = tool_result.get("success", False)
                llava_answer = tool_result.get("answer", "")
                
                self.logger.info(f"[VQA] LLaVA completed: success={llava_success}, answer_length={len(llava_answer)}")
                
                # Quality check
                if llava_success and llava_answer:
                    meaningless_indicators = [
                        "I cannot", "I am not able", "I don't have", 
                        "cannot analyze", "unable to", "no information"
                    ]
                    
                    is_meaningless = any(indicator in llava_answer.lower() for indicator in meaningless_indicators)
                    is_too_short = len(llava_answer.strip()) < 50
                    
                    if is_meaningless or is_too_short:
                        self.logger.warning(f"[VQA] Poor quality response detected")
                        llava_success = False
                        tool_result["success"] = False
                        tool_result["error"] = "LLaVA response quality insufficient"
        
        # SAFETY CHECK: If LLaVA failed, return error immediately
        if not llava_success:
            self.logger.error("[VQA] LLaVA failed - returning safety error")
            return {
                **state,
                "vqa_result": {
                    "success": False,
                    "error": "LLaVA medical consultation failed",
                    "answer": "âŒ **Lá»—i há»‡ thá»‘ng tÆ° váº¥n y táº¿**\n\nHá»‡ thá»‘ng LLaVA gáº·p sá»± cá»‘ vÃ  khÃ´ng thá»ƒ thá»±c hiá»‡n tÆ° váº¥n an toÃ n.\n\nðŸ¥ **Khuyáº¿n nghá»‹:** Vui lÃ²ng thá»­ láº¡i hoáº·c tham kháº£o bÃ¡c sÄ© chuyÃªn khoa trá»±c tiáº¿p.",
                    "safety_action": "Refused to generate medical advice when tool failed"
                }
            }
        
        # LLaVA succeeded - proceed with synthesis
        self.logger.info("[VQA] LLaVA succeeded - proceeding with synthesis")
        
        synthesis_text = f"""**MEDICAL CONSULTATION SYNTHESIS**

Task: Query="{task_input.get('query')}", Image={bool(task_input.get('image_path'))}, Text-only={task_input.get('is_text_only', False)}

**LLaVA Tool Status: âœ… SUCCESS**
LLaVA Response: {results['llava_vqa'].get('answer', '')[:200]}...

{self._format_synthesis_input()}
"""
        
        synthesis_messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=synthesis_text)
        ]
            
        # Invoke LLM for synthesis
        synthesis_response = self.llm.invoke(synthesis_messages)
        
        # Return agent result
        agent_result = self._extract_agent_result(synthesis_response.content)
        return {**state, **agent_result}